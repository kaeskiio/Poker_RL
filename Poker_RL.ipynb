{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tgrzMCcWILgS"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## RL\n",
        "In this project, you'll train two kinds of RL agents (DQN and CFR) on No-limit Texas Holdem Poker and a simpler version of poker called le-duc holdem."
      ],
      "metadata": {
        "id": "tgrzMCcWILgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install rlcard[torch]"
      ],
      "metadata": {
        "id": "D2dDilvHrP2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF8fQrFRlI1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le-Duc Hold Em"
      ],
      "metadata": {
        "id": "JAuI4SaUrv8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rlcard\n",
        "from rlcard import models\n",
        "from rlcard.agents import LeducholdemHumanAgent as HumanAgent\n",
        "from rlcard.utils import print_card\n",
        "\n",
        "# Make environment\n",
        "env = rlcard.make('leduc-holdem')\n",
        "human_agent = HumanAgent(env.num_actions)\n",
        "cfr_agent = models.load('leduc-holdem-cfr').agents[0]\n",
        "env.set_agents([\n",
        "    human_agent,\n",
        "    cfr_agent,\n",
        "])\n",
        "\n",
        "print(\">> Leduc Hold'em pre-trained model\")\n",
        "\n",
        "while (True):\n",
        "    print(\">> Start a new game\")\n",
        "\n",
        "    trajectories, payoffs = env.run(is_training=False)\n",
        "    final_state = trajectories[0][-1]\n",
        "    action_record = final_state['action_record']\n",
        "    state = final_state['raw_obs']\n",
        "    _action_list = []\n",
        "    for i in range(1, len(action_record)+1):\n",
        "        if action_record[-i][0] == state['current_player']:\n",
        "            break\n",
        "        _action_list.insert(0, action_record[-i])\n",
        "    for pair in _action_list:\n",
        "        print('>> Player', pair[0], 'chooses', pair[1])\n",
        "\n",
        "    # Let's take a look at what the agent card is\n",
        "    print('===============     CFR Agent    ===============')\n",
        "    print_card(env.get_perfect_information()['hand_cards'][1])\n",
        "\n",
        "    print('===============     Result     ===============')\n",
        "    if payoffs[0] > 0:\n",
        "        print('You win {} chips!'.format(payoffs[0]))\n",
        "    elif payoffs[0] == 0:\n",
        "        print('It is a tie.')\n",
        "    else:\n",
        "        print('You lose {} chips!'.format(-payoffs[0]))\n",
        "    print('')\n",
        "\n",
        "    input(\"Press any key to continue...\")"
      ],
      "metadata": {
        "id": "vyrLaDKElMSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "7-beqoDhvSqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "\n",
        "import rlcard\n",
        "from rlcard.agents import (\n",
        "    CFRAgent,\n",
        "    RandomAgent,\n",
        "    DQNAgent\n",
        ")\n",
        "from rlcard.utils import (\n",
        "    set_seed,\n",
        "    tournament,\n",
        "    Logger,\n",
        "    plot_curve,\n",
        "    reorganize,\n",
        ")\n",
        "\n",
        "def train(agent_type, game_type = 'leduc-holdem', seed = 42, num_episodes = 5000, evaluate_every = 100, num_eval_games = 2000, log_dir = \"cfr/\"):\n",
        "    env = rlcard.make(\n",
        "        game_type,\n",
        "        config={\n",
        "            'seed': 0,\n",
        "            'allow_step_back': True,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    eval_env = rlcard.make(\n",
        "        game_type,\n",
        "        config={\n",
        "            'seed': 0,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Seed numpy, torch, random\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Initilize CFR Agent\n",
        "    if agent_type == \"cfr\":\n",
        "      agent = CFRAgent(\n",
        "          env,\n",
        "          os.path.join(\n",
        "              log_dir,\n",
        "              'cfr_model',\n",
        "          ),\n",
        "      )\n",
        "\n",
        "      agent.load()  # If we have saved model, we first load the model\n",
        "    elif agent_type == \"dqn\":\n",
        "\n",
        "      agent = DQNAgent(\n",
        "          num_actions=env.num_actions,\n",
        "          state_shape=env.state_shape[0],\n",
        "          mlp_layers=[64,64],\n",
        "          save_path = log_dir,\n",
        "          save_every = num_episodes / 5,\n",
        "      )\n",
        "\n",
        "      env.set_agents([agent, agent])\n",
        "\n",
        "    # Evaluate Agent against random\n",
        "    eval_env.set_agents([\n",
        "        agent,\n",
        "        RandomAgent(num_actions=env.num_actions),\n",
        "    ])\n",
        "\n",
        "    # Start training\n",
        "    with Logger(log_dir) as logger:\n",
        "        for episode in range(num_episodes):\n",
        "            if agent_type == \"cfr\":\n",
        "              agent.train()\n",
        "            elif agent_type == \"dqn\":\n",
        "              trajectories, payoffs = env.run(is_training=True)\n",
        "\n",
        "              # Reorganaize the data to be state, action, reward, next_state, done\n",
        "              trajectories = reorganize(trajectories, payoffs)\n",
        "\n",
        "              # Feed transitions into agent memory, and train the agent\n",
        "              for ts in trajectories[0]:\n",
        "                  agent.feed(ts)\n",
        "\n",
        "            print('\\rIteration {}'.format(episode), end='')\n",
        "            # Evaluate the performance. Play with Random agents.\n",
        "            if episode % evaluate_every == 0:\n",
        "                if agent_type == \"cfr\":\n",
        "                  agent.save() # Save model\n",
        "\n",
        "                logger.log_performance(\n",
        "                    episode,\n",
        "                    tournament(\n",
        "                        eval_env,\n",
        "                        num_eval_games\n",
        "                    )[0]\n",
        "                )\n",
        "\n",
        "        # Get the paths\n",
        "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
        "    # Plot the learning curve\n",
        "    plot_curve(csv_path, fig_path, agent_type)\n",
        "\n",
        "\n",
        "agent_type = \"dqn\"\n",
        "game_type = \"no-limit-holdem\"\n",
        "train(agent_type, game_type = game_type, num_episodes = 2000, log_dir = agent_type + \"-\" + game_type + \"/\")"
      ],
      "metadata": {
        "id": "zw0swxf4t9DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_env = rlcard.make(\n",
        "    'leduc-holdem',\n",
        "    config={\n",
        "        'seed': 0,\n",
        "    }\n",
        ")\n",
        "\n",
        "pretrained_agent = models.load('leduc-holdem-cfr').agents[0]\n",
        "\n",
        "our_dqn_agent = DQNAgent(\n",
        "          num_actions=eval_env.num_actions,\n",
        "          state_shape=eval_env.state_shape[0],\n",
        "          mlp_layers=[64,64],\n",
        "          save_path = \"dqn/\",\n",
        "          )\n",
        "\n",
        "checkpoint = torch.load(\"dqn/checkpoint_dqn.pt\")\n",
        "our_dqn_agent.from_checkpoint(checkpoint)\n",
        "\n",
        "our_cfr_agent = CFRAgent(\n",
        "    eval_env,\n",
        "    os.path.join(\n",
        "        \"cfr/\",\n",
        "        'cfr_model',\n",
        "    ),\n",
        ")\n",
        "\n",
        "our_cfr_agent.load()\n",
        "\n",
        "eval_env.set_agents([\n",
        "    our_dqn_agent,\n",
        "    our_cfr_agent,\n",
        "])\n",
        "\n",
        "out = tournament(eval_env, 1000)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "oKPK2sIgr0bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play No-Limit Texas Hold-em against your AI"
      ],
      "metadata": {
        "id": "dxAADUTW73Vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rlcard\n",
        "from rlcard import models\n",
        "from rlcard.agents import NolimitholdemHumanAgent\n",
        "from rlcard.utils import print_card\n",
        "\n",
        "# Make environment\n",
        "env = rlcard.make('no-limit-holdem')\n",
        "\n",
        "human_agent = NolimitholdemHumanAgent(env.num_actions)\n",
        "\n",
        "our_dqn_agent = DQNAgent(\n",
        "          num_actions=env.num_actions,\n",
        "          state_shape=env.state_shape[0],\n",
        "          mlp_layers=[64,64],\n",
        "          save_path = \"dqn/\",\n",
        "          )\n",
        "\n",
        "checkpoint = torch.load(\"dqn-no-limit-holdem/checkpoint_dqn.pt\")\n",
        "our_dqn_agent.from_checkpoint(checkpoint)\n",
        "\n",
        "# cfr_agent = models.load('leduc-holdem-cfr').agents[0]\n",
        "env.set_agents([\n",
        "    our_dqn_agent,\n",
        "    human_agent,\n",
        "])\n",
        "\n",
        "print(\">> Play No-Limit Texas Holdem against a random agent\")\n",
        "\n",
        "while (True):\n",
        "    print(\">> Start a new game\")\n",
        "\n",
        "    trajectories, payoffs = env.run(is_training=False)\n",
        "    # If the human does not take the final action, we need to\n",
        "    # print other players action\n",
        "    print(trajectories)\n",
        "    final_state = trajectories[0][-1]\n",
        "    action_record = final_state['action_record']\n",
        "    state = final_state['raw_obs']\n",
        "    _action_list = []\n",
        "    for i in range(1, len(action_record)+1):\n",
        "        if action_record[-i][0] == state['current_player']:\n",
        "            break\n",
        "        _action_list.insert(0, action_record[-i])\n",
        "    for pair in _action_list:\n",
        "        print('>> Player', pair[0], 'chooses', pair[1])\n",
        "\n",
        "    # Let's take a look at what the agent card is\n",
        "    print('===============     DQN Agent    ===============')\n",
        "    print_card(env.get_perfect_information()['hand_cards'][0])\n",
        "\n",
        "    print('===============     Result     ===============')\n",
        "    if payoffs[0] > 0:\n",
        "        print('You win {} chips!'.format(payoffs[0]))\n",
        "    elif payoffs[0] == 0:\n",
        "        print('It is a tie.')\n",
        "    else:\n",
        "        print('You lose {} chips!'.format(-payoffs[0]))\n",
        "    print('')\n",
        "\n",
        "    input(\"Press any key to continue...\")"
      ],
      "metadata": {
        "id": "GR8J_j1175SB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}